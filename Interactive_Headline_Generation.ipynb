{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAw8rui4D8Lr3m6kvorl0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santheartist/small.projects/blob/main/Interactive_Headline_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "c7YRU5F6WMD7",
        "outputId": "e0d7469b-f70f-468b-fff9-5860be8f35c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully with 102915 records.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,024,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m), │    \u001b[38;5;34m131,584\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,024,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bahdanau_attention  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m33,153\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)]    │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m), │    \u001b[38;5;34m131,584\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bahdanau_attenti… │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m8000\u001b[0m)  │  \u001b[38;5;34m2,056,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bahdanau_attention  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,153</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]    │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bahdanau_attenti… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,400,321\u001b[0m (16.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,400,321</span> (16.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,400,321\u001b[0m (16.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,400,321</span> (16.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 27ms/step - loss: 5.7292 - val_loss: 4.8556\n",
            "Epoch 2/5\n",
            "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 27ms/step - loss: 4.6453 - val_loss: 4.2555\n",
            "Epoch 3/5\n",
            "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 28ms/step - loss: 4.0454 - val_loss: 3.9372\n",
            "Epoch 4/5\n",
            "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - loss: 3.6780 - val_loss: 3.7575\n",
            "Epoch 5/5\n",
            "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - loss: 3.3836 - val_loss: 3.6445\n",
            "<OOV>\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# 📌 Headline Generation using LSTM Variants\n",
        "# ===============================================\n",
        "\n",
        "# STEP 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Layer, Concatenate, GlobalAveragePooling1D, RepeatVector\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ===============================================\n",
        "# STEP 2: Load Dataset with Error Handling for Malformed Rows\n",
        "# ===============================================\n",
        "try:\n",
        "    df1 = pd.read_csv('/content/news_summary.csv', encoding='ISO-8859-1', on_bad_lines='skip')[['text', 'headlines']]\n",
        "    df2 = pd.read_csv('/content/news_summary_more.csv', encoding='ISO-8859-1', engine='python', on_bad_lines='skip')[['text', 'headlines']]\n",
        "    df = pd.concat([df1, df2]).dropna().reset_index(drop=True)\n",
        "    if df.empty:\n",
        "        raise ValueError(\"The dataset is empty after loading.\")\n",
        "    print(f\"Dataset loaded successfully with {len(df)} records.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "# ===============================================\n",
        "# STEP 3: Clean Text\n",
        "# ===============================================\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "df['clean_headlines'] = df['headlines'].apply(clean_text)\n",
        "\n",
        "# ===============================================\n",
        "# STEP 4: Tokenization and Padding\n",
        "# ===============================================\n",
        "VOCAB_SIZE = 8000\n",
        "MAX_INPUT_LEN = 50\n",
        "MAX_TARGET_LEN = 12\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "input_tokenizer.fit_on_texts(df['clean_text'])\n",
        "input_seq = input_tokenizer.texts_to_sequences(df['clean_text'])\n",
        "input_seq = pad_sequences(input_seq, maxlen=MAX_INPUT_LEN, padding='post')\n",
        "\n",
        "target_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "target_tokenizer.fit_on_texts(df['clean_headlines'])\n",
        "target_seq = target_tokenizer.texts_to_sequences(df['clean_headlines'])\n",
        "target_seq = pad_sequences(target_seq, maxlen=MAX_TARGET_LEN, padding='post')\n",
        "\n",
        "decoder_input_seq = target_seq[:, :-1]\n",
        "decoder_target_seq = target_seq[:, 1:]\n",
        "decoder_target_seq = np.expand_dims(decoder_target_seq, -1)\n",
        "\n",
        "X_train, X_val, Xd_train, Xd_val, Y_train, Y_val = train_test_split(\n",
        "    input_seq, decoder_input_seq, decoder_target_seq, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# ===============================================\n",
        "# STEP 5A: Vanilla LSTM Encoder-Decoder\n",
        "# ===============================================\n",
        "def build_lstm_model():\n",
        "    encoder_inputs = Input(shape=(MAX_INPUT_LEN,))\n",
        "    enc_emb = Embedding(VOCAB_SIZE, 128)(encoder_inputs)\n",
        "    enc_out, state_h, state_c = LSTM(128, return_state=True)(enc_emb)\n",
        "\n",
        "    decoder_inputs = Input(shape=(MAX_TARGET_LEN - 1,))\n",
        "    dec_emb = Embedding(VOCAB_SIZE, 128)(decoder_inputs)\n",
        "    dec_lstm, _, _ = LSTM(128, return_sequences=True, return_state=True)(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    output = Dense(VOCAB_SIZE, activation='softmax')(dec_lstm)\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "# ===============================================\n",
        "# STEP 5B: LSTM with Bahdanau Attention (Fixed)\n",
        "# ===============================================\n",
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "def build_attention_model():\n",
        "    encoder_inputs = Input(shape=(MAX_INPUT_LEN,))\n",
        "    enc_emb = Embedding(VOCAB_SIZE, 128)(encoder_inputs)\n",
        "    enc_out, state_h, state_c = LSTM(128, return_sequences=True, return_state=True)(enc_emb)\n",
        "\n",
        "    decoder_inputs = Input(shape=(MAX_TARGET_LEN - 1,))\n",
        "    dec_emb = Embedding(VOCAB_SIZE, 128)(decoder_inputs)\n",
        "    dec_lstm_out, _, _ = LSTM(128, return_sequences=True, return_state=True)(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    attention = BahdanauAttention(128)\n",
        "    context_vector, _ = attention(state_h, enc_out)\n",
        "    repeated_context = RepeatVector(MAX_TARGET_LEN - 1)(context_vector)\n",
        "    concat = Concatenate(axis=-1)([dec_lstm_out, repeated_context])\n",
        "    output = Dense(VOCAB_SIZE, activation='softmax')(concat)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "# ===============================================\n",
        "# STEP 5C: LSTM with Self-Attention (Fixed)\n",
        "# ===============================================\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.Wq = Dense(units)\n",
        "        self.Wk = Dense(units)\n",
        "        self.Wv = Dense(units)\n",
        "\n",
        "    def call(self, x):\n",
        "        q = self.Wq(x)\n",
        "        k = self.Wk(x)\n",
        "        v = self.Wv(x)\n",
        "\n",
        "        score = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(tf.shape(k)[-1], tf.float32))\n",
        "        weights = tf.nn.softmax(score, axis=-1)\n",
        "        output = tf.matmul(weights, v)\n",
        "        return output\n",
        "\n",
        "def build_self_attention_model():\n",
        "    encoder_inputs = Input(shape=(MAX_INPUT_LEN,))\n",
        "    enc_emb = Embedding(VOCAB_SIZE, 128)(encoder_inputs)\n",
        "    enc_out, state_h, state_c = LSTM(128, return_sequences=True, return_state=True)(enc_emb)\n",
        "\n",
        "    encoder_sa = SelfAttention(128)(enc_out)\n",
        "    encoder_pooled = GlobalAveragePooling1D()(encoder_sa)\n",
        "    encoder_repeated = RepeatVector(MAX_TARGET_LEN - 1)(encoder_pooled)\n",
        "\n",
        "    decoder_inputs = Input(shape=(MAX_TARGET_LEN - 1,))\n",
        "    dec_emb = Embedding(VOCAB_SIZE, 128)(decoder_inputs)\n",
        "    dec_lstm_out, _, _ = LSTM(128, return_sequences=True, return_state=True)(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    concat = Concatenate(axis=-1)([dec_lstm_out, encoder_repeated])\n",
        "    output = Dense(VOCAB_SIZE, activation='softmax')(concat)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "# ===============================================\n",
        "# STEP 6: Train Self-Attention Model (Example)\n",
        "# ===============================================\n",
        "model = build_attention_model()\n",
        "model.summary()\n",
        "model.fit([X_train, Xd_train], Y_train, validation_data=([X_val, Xd_val], Y_val), batch_size=64, epochs=5)\n",
        "\n",
        "# ===============================================\n",
        "# STEP 7: Inference Function\n",
        "# ===============================================\n",
        "def generate_headline(input_text, model, input_tokenizer, target_tokenizer):\n",
        "    input_seq = input_tokenizer.texts_to_sequences([clean_text(input_text)])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=MAX_INPUT_LEN, padding='post')\n",
        "\n",
        "    decoder_input = np.zeros((1, MAX_TARGET_LEN - 1))\n",
        "    decoded_words = []\n",
        "\n",
        "    for i in range(MAX_TARGET_LEN - 1):\n",
        "        prediction = model.predict([input_seq, decoder_input], verbose=0)\n",
        "        sampled_token_index = np.argmax(prediction[0, i, :])\n",
        "        sampled_word = target_tokenizer.index_word.get(sampled_token_index, '')\n",
        "        if sampled_word == '':\n",
        "            break\n",
        "        decoded_words.append(sampled_word)\n",
        "        decoder_input[0, i] = sampled_token_index\n",
        "\n",
        "    return ' '.join(decoded_words)\n",
        "\n",
        "# ✅ Example usage\n",
        "print(generate_headline(\"NASA has launched a new satellite for Earth observation purposes.\", model, input_tokenizer, target_tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# ✅ STEP 8: Interactive Headline Generation via User Input\n",
        "# ===============================================\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\n📥 Enter a news article (or type 'exit' to stop):\\n\")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"👋 Exiting headline generator.\")\n",
        "        break\n",
        "    headline = generate_headline(user_input, model, input_tokenizer, target_tokenizer)\n",
        "    print(f\"\\n📰 Generated Headline: {headline}\")\n"
      ],
      "metadata": {
        "id": "V1Jr2vqQsRWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}